import pandas as pd
import json
import time
from sklearn.preprocessing import MinMaxScaler
import ssl
import certifi
from google import genai
import streamlit as st
from io import BytesIO
from pathlib import Path
import requests
import os
import concurrent.futures

API_KEYS = [
    "AIzaSyAHuh1NvFx0arcKMm6RbP1EF94TsRvjHh4",
    "AIzaSyDTUCWsjAj1t9D3Tnix6hHWJ3rxjzfleMs",
    "AIzaSyB0cYSO6BEXFY4EGz9A7lzdPMWw84peOXU",
    "AIzaSyAjpckNYaZkifXUdvTL0QpyAVL8k6fsSzw",
    "AIzaSyBrNZg8x4UZfFo3yVmCXJa2YeHG1iFiVhk",
    "AIzaSyATYtdvAxyBetlg00zrX8CKiVZRxfeuHtM",
    "AIzaSyB5iV3q2APEh27EJ7Jm6qqBeIfGnauqdiw",
    "AIzaSyC8tN4kY2QU5ACRacPazzRQeJPtAC08Vm8",
    "AIzaSyCh78GHgYJ9DZUo-TekO6MDxpMWTHD_zjk",
    "AIzaSyBww4mPXQz2dStjJcQJ-T6DAvXXUpgkBBo"
]

# --- ØªØ§Ø¨Ø¹ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² APIÙ‡Ø§ ---
def safe_generate_content(*, model, contents, config):
    for api_key in API_KEYS:
        try:
            client = genai.Client(api_key=api_key)
            response = client.models.generate_content(
                model=model,
                contents=contents,
                config=config
            )
            return response
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¨Ø§ API {api_key[:10]}...: {str(e)}")
            continue
    raise RuntimeError("âŒ ØªÙ…Ø§Ù… API KeyÙ‡Ø§ Ø¨Ø§ Ø®Ø·Ø§ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯Ù†Ø¯.")




# --- Ø­Ø°Ù ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ Ù‚Ø¨Ù„ÛŒ ---
RESULT_FILE_PATH = Path("resume_results.xlsx")
if RESULT_FILE_PATH.exists():
    RESULT_FILE_PATH.unlink()

# --- Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ session Ø¯Ø± Ù‡Ø± Ø¨Ø§Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ ---
#for key in ['final_df', 'live_results']:
    #if key in st.session_state:
        #del st.session_state[key]

# --- Ú¯ÙˆØ§Ù‡ÛŒ SSL ---
os.environ['SSL_CERT_FILE'] = certifi.where()

# --- ØªÙ†Ø¸ÛŒÙ… Ù¾Ø±Ø§Ú©Ø³ÛŒ ---
proxy_url = "http://localhost:2080"
os.environ['HTTP_PROXY'] = proxy_url
os.environ['HTTPS_PROXY'] = proxy_url

# --- ØªØ³Øª Ø§ØªØµØ§Ù„ ---
test_url = "https://generativelanguage.googleapis.com/v1beta/models"
try:
    response = requests.get(test_url, proxies={"http": proxy_url, "https": proxy_url}, timeout=5)
    if response.status_code == 200:
        print("âœ… Ø§ØªØµØ§Ù„ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø§Ø³Øª.")
    else:
        print(f"âš ï¸ Ú©Ø¯ ÙˆØ¶Ø¹ÛŒØª: {response.status_code}")
except Exception as e:
    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ù¾Ø±Ø§Ú©Ø³ÛŒ: {e}")

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ ---
pd.set_option('display.max_rows', None)
OUTPUT_ALL_PATH = Path("recruitment_score.xlsx")
ID_COLUMN = 'Ø´Ù†Ø§Ø³Ù‡'
BATCH_SIZE = 10
RESULT_FILE_PATH = Path("resume_results.xlsx")

if 'live_results' not in st.session_state:
    st.session_state['live_results'] = []


# --- ØªÙ… Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ ---
streamlit_style = """
<style>
@font-face {
    font-family: 'BNazanin';
    src: url('https://cdn.fontcdn.ir/Fonts/B%20Nazanin/B%20Nazanin.woff2') format('woff2'),
         url('https://cdn.fontcdn.ir/Fonts/B%20Nazanin/B%20Nazanin.woff') format('woff');
    font-weight: normal;
    font-style: normal;
}

main, .block-container {
    direction: rtl;
    text-align: right;
    font-family: 'BNazanin', Tahoma, 'IRANSansWeb', sans-serif !important;
    font-size: 14px;
    line-height: 1.8;
}

html, body {
    font-family: 'BNazanin', Tahoma, 'IRANSansWeb', sans-serif  !important;
    background-color: #f5f7fa;
    color: #333333;
}

h1 {
    font-size: 42px;
    font-weight: bold;
    color: #1a73e8;
    text-align: right;
}

h2 {
    font-size: 32px;
    font-weight: bold;
    color: #1a73e8;
    text-align: right;
}

h3 {
    font-size: 30px;
    font-weight: bold;
    color: #1a73e8;
    text-align: right;
}

.stButton>button, .stDownloadButton>button {
    background-color: #1a73e8;
    color: white;
    border: none;
    padding: 12px 24px;
    border-radius: 8px;
    font-size: 16px;
    font-weight: bold;
    font-family: 'BNazanin', Tahoma, 'IRANSansWeb', sans-serif !important;
}

footer {visibility: hidden;}
</style>
"""



st.markdown(streamlit_style, unsafe_allow_html=True)

# --- Ù„ÛŒØ³Øª Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø´ØºÙ„ÛŒ ---
JOB_PROFILES = [
    {
        "id": "job_rnd_01",
        "title": "ØªØ­Ù‚ÛŒÙ‚ Ùˆ ØªÙˆØ³Ø¹Ù‡ Ø³Ø§Ù…Ø§Ù†Ù‡â€ŒÙ‡Ø§",
        "tasks": [
            "ØªØ­Ù„ÛŒÙ„ Ùˆ Ø§Ø­ØµØ§ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±ÛŒ Ø°ÛŒÙ†ÙØ¹Ø§Ù† Ø¯Ø± Ø­ÙˆØ²Ù‡ Ø²ÛŒØ±Ø³Ø§Ø®Øªâ€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø­Ø¬ÛŒÙ…ØŒ Ù„Ø§Ú¯ Ùˆ Ú¯Ø±Ø¯Ø´ Ú©Ø§Ø±",
            "ØªÙˆØ³Ø¹Ù‡ Ùˆ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø±Ø§Ù‡Ú©Ø§Ø±Ù‡Ø§ÛŒ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±ÛŒ Ø´Ø§Ù…Ù„ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒØŒ Ú¯Ø±Ø¯Ø´ Ú©Ø§Ø± Ùˆ Ù¾Ø±ØªØ§Ù„",
            "Ø§Ø¬Ø±Ø§ÛŒ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø²ÛŒØ±Ø³Ø§Ø®Øª Ùˆ Ø³Ø§Ù…Ø§Ù†Ù‡â€ŒÙ‡Ø§",
            "Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ùˆ Ù†Ú¯Ù‡Ø¯Ø§Ø´Øª Ø³Ø§Ù…Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ùˆ Ù¾Ø§Ø³Ø® Ø¨Ù‡ ØªÛŒÚ©Øªâ€ŒÙ‡Ø§",
            "Ù…Ø³ØªÙ†Ø¯Ø³Ø§Ø²ÛŒ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ù†Ø´ Ø³Ø§Ù…Ø§Ù†Ù‡â€ŒÙ‡Ø§"
        ],
        "competencies_technical": [
            {"name": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ùˆ Ù…Ø¨Ø§Ù†ÛŒ Ø¹Ù„Ù… Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±"},
            {"name": "ØªØ­Ù„ÛŒÙ„ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±"},
            {"name": "Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ø¨Ú©â€ŒØ§Ù†Ø¯ ÛŒØ§ ÙØ±Ø§Ù†Øª (Ù…Ø«Ù„ Python ÛŒØ§ JavaScript)"},
            {"name": "Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡"}
        ],
        "majors": ["Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±", "Ù…Ù‡Ù†Ø¯Ø³ÛŒ ØµÙ†Ø§ÛŒØ¹", "Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ÛŒ ÙÙ†ÛŒ Ùˆ Ù…Ù‡Ù†Ø¯Ø³ÛŒ"]
    },
    {
        "id": "job_spatial_01",
        "title": "ØªÙˆØ³Ø¹Ù‡ Ø±Ø§Ù‡Ú©Ø§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ú©Ø§Ù†ÛŒ",
        "tasks": [
            "ØªØ­Ù„ÛŒÙ„ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ…Ø­ÙˆØ± Ù…Ú©Ø§Ù†ÛŒ",
            "ØªÙˆØ³Ø¹Ù‡ Ø±Ø§Ù‡Ú©Ø§Ø±Ù‡Ø§ÛŒ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±ÛŒ GIS Ùˆ RS",
            "ÙØ±Ø§ÛŒÙ†Ø¯ ETL Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ú©Ø§Ù†ÛŒ",
            "Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ùˆ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø±Ø§Ù‡Ú©Ø§Ø±Ù‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ…Ø­ÙˆØ± GIS/RS",
            "Ù…Ø³ØªÙ†Ø¯Ø³Ø§Ø²ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ Ùˆ Ù…Ø§Ù…ÙˆØ±ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ú©Ø§Ù†ÛŒ"
        ],
        "competencies_technical": [
            {"name": "Ù…Ø¨Ø§Ù†ÛŒ Ø³Ù†Ø¬Ø´ Ø§Ø² Ø¯ÙˆØ±"},
            {"name": "Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ RS Ù…Ø§Ù†Ù†Ø¯ ENVIØŒ ERDASØŒ SNAP"},
            {"name": "Ù…Ø¨Ø§Ù†ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ / Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ±"},
            {"name": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Python / MATLAB"},
            {"name": "Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±Ù‡Ø§ÛŒ GIS Ù…Ø§Ù†Ù†Ø¯ ArcGIS/QGIS"}
        ],
        "majors": ["Ù†Ù‚Ø´Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ", "Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±", "Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ø¨Ø±Ù‚"]
    },
    {
        "id": "job_ai_01",
        "title": "ØªÙˆØ³Ø¹Ù‡ Ø±Ø§Ù‡Ú©Ø§Ø±Ù‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ",
        "tasks": [
            "ØªØ­Ù„ÛŒÙ„ Ù†ÛŒØ§Ø²Ù‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ…Ø­ÙˆØ± Ø¨Ø§ ØªØ§Ú©ÛŒØ¯ Ø¨Ø± AI",
            "Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†",
            "Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ MLOps",
            "ØªÙ‡ÛŒÙ‡ Ú¯Ø²Ø§Ø±Ø´Ø§Øª ØªØ­Ù„ÛŒÙ„ÛŒ",
            "Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ù†Ø´ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ AI"
        ],
        "competencies_technical": [
            {"name": "Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¢Ù…Ø§Ø±ÛŒ / ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†"},
            {"name": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Python / R / GAMS"},
            {"name": "Ú©Ø§Ø± Ø¨Ø§ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡"}
        ],
        "majors": ["Ø¹Ù„ÙˆÙ… Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±", "Ø±ÛŒØ§Ø¶ÛŒ", "Ø¢Ù…Ø§Ø±", "Ù…Ù‡Ù†Ø¯Ø³ÛŒ ØµÙ†Ø§ÛŒØ¹", "Ø§Ù‚ØªØµØ§Ø¯", "Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ù…Ø§Ù„ÛŒ", "Ø¨Ø±Ù‚"]
    },
    {
        "id": "job_research_01",
        "title": "Ú©Ø§Ø±Ø´Ù†Ø§Ø³ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ù…Ø±Ø§Ú©Ø² Ù¾Ú˜ÙˆÙ‡Ø´ÛŒ",
        "tasks": [
            "Ø§Ø­ØµØ§ Ù…Ø³Ø§Ø¦Ù„ ÙÙ†Ø§ÙˆØ±Ø§Ù†Ù‡ Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ…Ø­ÙˆØ±",
            "Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø³Ø§Ù…Ø§Ù†Ù‡ Ø¯Ø§Ù†Ø´ Ù†Ø¸Ø§Ù… Ù…Ø³Ø§Ø¦Ù„",
            "Ù…Ø·Ø§Ù„Ø¹Ø§Øª ØªØ·Ø¨ÛŒÙ‚ÛŒ Ø¯Ø± Ø­ÙˆØ²Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ",
            "Ù…Ø³ØªÙ†Ø¯Ø³Ø§Ø²ÛŒ Ø§Ø³Ù†Ø§Ø¯ Ø±Ø§Ù‡Ø¨Ø±Ø¯ÛŒ AI",
            "Ø±ØµØ¯ Ùˆ ØªØ­Ù„ÛŒÙ„ ÙÙ†Ø§ÙˆØ±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†ÙˆØ¸Ù‡ÙˆØ±"
        ],
        "competencies_technical": [
            {"name": "Microsoft Office"},
            {"name": "Ù…Ø¨Ø§Ù†ÛŒ Ø¹Ù„Ù… Ø¯Ø§Ø¯Ù‡ Ùˆ IT"},
            {"name": "Ø§ØµÙˆÙ„ ØªØ­Ù‚ÛŒÙ‚ Ùˆ ØªÙˆØ³Ø¹Ù‡"}
        ],
        "majors": ["Ù…Ø¯ÛŒØ±ÛŒØª", "Ù…Ù‡Ù†Ø¯Ø³ÛŒ ØµÙ†Ø§ÛŒØ¹", "Ø¹Ù„ÙˆÙ… Ø§Ù‚ØªØµØ§Ø¯ÛŒ", "Ø¹Ù„ÙˆÙ… Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±"]
    },
    {
        "id": "job_analysis_01",
        "title": "Ú©Ø§Ø±Ø´Ù†Ø§Ø³ ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ø¯Ø§Ø¯Ù‡ Ùˆ Ù‡ÙˆØ´ ØªØ¬Ø§Ø±ÛŒ",
        "tasks": [
            "Ú¯Ø±ÙˆÙ‡ Ø¨Ù†Ø¯ÛŒ Ùˆ Ù…Ø±ØªØ¨ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª",
            "ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ú©Ø³Ø¨ Ùˆ Ú©Ø§Ø±",
            "ØªÙ…ÛŒØ²Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ ETL",
            "Ù…Ø³ØªÙ†Ø¯Ø³Ø§Ø²ÛŒ Ø§Ø³Ù†Ø§Ø¯ Ø±Ø§Ù‡Ø¨Ø±Ø¯ÛŒ ",
            "Ù†Ø§Ù…Ù‡ Ù†Ú¯Ø§Ø±ÛŒ Ùˆ Ù…Ú©Ø§ØªØ¨Ø§Øª Ø§Ø¯Ø§Ø±ÛŒ",
            "Ø¨ØµØ±ÛŒ Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§"
        ],
        "competencies_technical": [
            {"name": "Microsoft Office"},
            {"name": "Ø´Ù†Ø§Ø®Øª Ùˆ ØªØ­Ù„ÛŒÙ„ Ú©Ø³Ø¨ Ùˆ Ú©Ø§Ø±"},
            {"name": "Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù…ØµÙˆØ±Ø³Ø§Ø²ÛŒ Ù…Ø§Ù†Ù†Ø¯ powerBI"},
            {"name": "Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ù…Ø§Ù†Ù†Ø¯ KNIME"},
            {"name": "Ø¢Ø´Ù†Ø§ÛŒÛŒ Ø¨Ø§ Ø²Ø§Ù† Ù‡Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù†ÙˆÛŒØ³ÛŒ Ù…Ø§Ù†Ù†Ø¯ python , R"}
        ],
        "majors": ["Ù…Ø¯ÛŒØ±ÛŒØª", "Ù…Ù‡Ù†Ø¯Ø³ÛŒ ØµÙ†Ø§ÛŒØ¹", "Ø¹Ù„ÙˆÙ… Ø§Ù‚ØªØµØ§Ø¯ÛŒ", "Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±"]
    }
]



# --- ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ ---
def process_batch(batch_df, prompt_text):
    payload = {
        "employer requirements": prompt_text,
        "applicant information": [
            {"resume": " ".join([str(row[col]) for col in batch_df.columns]), "id": str(row[ID_COLUMN])}
            for _, row in batch_df.iterrows()
        ]
    }
    try:
        response = safe_generate_content(
            model='gemini-2.0-flash',
            contents=json.dumps(payload, ensure_ascii=False),
            config={
                'response_mime_type': 'application/json',
                'system_instruction': """
Ø´Ù…Ø§ ÛŒÚ© Ø§Ø±Ø²ÛŒØ§Ø¨ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ø§Ù†Ø³Ø§Ù†ÛŒ Ù‡Ø³ØªÛŒØ¯. Ù„Ø·ÙØ§Ù‹ Ø§Ù…ØªÛŒØ§Ø² Ø±Ø§ Ø¨Ø§ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø²Ø¦ÛŒ ØªØ¹ÛŒÛŒÙ† Ú©Ù†ÛŒØ¯.
Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ:
- Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±ÛŒ: ØªØ³Ù„Ø· Ú©Ø§Ù…Ù„ (Û³Û° Ø§Ù…ØªÛŒØ§Ø²)ØŒ Ø¢Ø´Ù†Ø§ÛŒÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ (Û±Ûµ)ØŒ Ù†Ø¨ÙˆØ¯ Ù…Ù‡Ø§Ø±Øª (Û°)
- Ø³ÙˆØ§Ø¨Ù‚ Ø´ØºÙ„ÛŒ Ù…Ø±ØªØ¨Ø·: Ú©Ø§Ù…Ù„ (Û²Û°)ØŒ Ø¬Ø²Ø¦ÛŒ (Û±Û°)ØŒ Ù†Ø§Ù…Ø±ØªØ¨Ø· (Û°)
- Ù…Ù‚Ø·Ø¹ Ùˆ Ø±Ø´ØªÙ‡ ØªØ­ØµÛŒÙ„ÛŒ: Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ Ù…Ø±ØªØ¨Ø· (Û±Ûµ)ØŒ Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ Ù†Ø§Ù…Ø±ØªØ¨Ø· (Ûµ)ØŒ Ø²ÛŒØ± Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ ÛŒØ§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†Ø§Ù‚Øµ (Û°)
- Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡: Ù…Ù…ØªØ§Ø² (Û±Û°)ØŒ Ø¯ÙˆÙ„ØªÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ (Ûµ)ØŒ ØºÛŒØ±Ø¯ÙˆÙ„ØªÛŒ (Û°)
- Ø³Ù† Ù…Ù†Ø§Ø³Ø¨: Û²Û² ØªØ§ Û³Ûµ Ø³Ø§Ù„ (Û±Û°)ØŒ Ú©Ù…ØªØ± ÛŒØ§ Ø¨ÛŒØ´ØªØ± (Û°)
- Ø­Ù‚ÙˆÙ‚ Ø¯Ø±Ø®ÙˆØ§Ø³ØªÛŒ Ù…Ù†Ø§Ø³Ø¨: Û²Û° ØªØ§ Û´Ûµ Ù…ÛŒÙ„ÛŒÙˆÙ† (Û±Û°)ØŒ Ø®Ø§Ø±Ø¬ Ø§Ø² Ø§ÛŒÙ† Ø¨Ø§Ø²Ù‡ (Û°)
Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ Ø±Ø§ Ø§Ø² Û° ØªØ§ Û±Û°Û° Ø¨Ø¯Ù‡ Ùˆ **Ø¯Ø± Ø­Ø¯ Ø§Ù…Ú©Ø§Ù†ØŒ ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø²Ø¦ÛŒ Ø±Ø§ Ù†ÛŒØ² Ù„Ø­Ø§Ø¸ Ú©Ù†.**
Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ù†Ø¨ÙˆØ¯ØŒ Ø¨Ù†ÙˆÛŒØ³: 'Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª'.
""",
                'response_schema': {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "score": {"type": "number", "nullable": False},
                            "check_id": {"type": "string", "nullable": False},
                            "why": {"type": "string", "nullable": False}
                        }
                    }
                },
                'temperature': 0
            }
        )
        result = json.loads(response.candidates[0].content.parts[0].text)
        

# ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„ÛŒØ¯ 'why' Ø¨Ù‡ 'Ø¯Ù„ÛŒÙ„'
        for item in result:
            if 'why' in item:
                item['Ø¯Ù„ÛŒÙ„'] = item.pop('why')

        return pd.DataFrame(result)

    except Exception:
        return pd.DataFrame([{
            "score": 1.0,
            "check_id": str(row[ID_COLUMN]),
            "Ø¯Ù„ÛŒÙ„": "Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒØ§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†Ø§Ú©Ø§ÙÛŒ"
        } for _, row in batch_df.iterrows()])


def to_excel(df, path):
    df.to_excel(path, index=False)

def match_resume_to_job_parallel(resume_text, job_profiles, threshold=7):
    best_match = None
    best_score = -1
    best_reason = ""
    log_messages = []

    def evaluate_job_with_key(api_key, job):
        prompt = f"""Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ±:
Ø±Ø²ÙˆÙ…Ù‡:
{resume_text}

Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ:
Ø¹Ù†ÙˆØ§Ù†: {job['title']}
Ø´Ø±Ø­ ÙˆØ¸Ø§ÛŒÙ: {'Ø› '.join(job['tasks'])}
Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ: {'Ø› '.join([c['name'] for c in job.get('competencies_technical', [])])}
Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·: {'Ø› '.join(job.get('majors', []))}

Ø¢ÛŒØ§ Ø§ÛŒÙ† Ø±Ø²ÙˆÙ…Ù‡ Ø¨Ø§ Ø§ÛŒÙ† Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ ØªØ·Ø§Ø¨Ù‚ Ø¯Ø§Ø±Ø¯ØŸ Ù„Ø·ÙØ§Ù‹:
- ÛŒÚ© Ø§Ù…ØªÛŒØ§Ø² Ø¨ÛŒÙ† Û° ØªØ§ 100 Ø¨Ø¯Ù‡
- Ø¯Ø± ØµÙˆØ±Øª Ù…Ù†Ø§Ø³Ø¨ Ø¨ÙˆØ¯Ù†ØŒ Ø¯Ù„ÛŒÙ„ Ø±Ø§ Ø´Ø±Ø­ Ø¨Ø¯Ù‡
- Ø¯Ø± ØµÙˆØ±Øª Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ Ø¨ÙˆØ¯Ù†ØŒ Ø¨Ù†ÙˆÛŒØ³ Ú†Ø±Ø§ Ù…Ù†Ø§Ø³Ø¨ Ù†ÛŒØ³Øª

Ù„Ø·ÙØ§Ù‹ Ù‡Ù…ÛŒØ´Ù‡ Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ÙØ±Ù…Øª Ø²ÛŒØ± Ùˆ Ø¨Ø§ Ù‡Ø±Ø¯Ùˆ Ø¨Ø®Ø´ Ø¨Ø¯Ù‡:
Ø§Ù…ØªÛŒØ§Ø²: [ÛŒÚ© Ø¹Ø¯Ø¯ Ø§Ø² 0 ØªØ§ 100]
Ø¯Ù„ÛŒÙ„: [ÛŒÚ© Ø¬Ù…Ù„Ù‡ ÙˆØ§Ø¶Ø­ Ùˆ Ø¯Ù‚ÛŒÙ‚ Ø´Ø§Ù…Ù„ Ø¯Ù„ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ ÛŒØ§ Ø¹Ø¯Ù… Ø§Ù†ØªØ®Ø§Ø¨]

"""
        try:
            response = safe_generate_content(
                model="gemini-2.0-flash",
                contents=prompt,
                config={"temperature": 0}
            )

            if isinstance(response, dict) and "error" in response:
                return None  # Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯Ù‡

            text = response.candidates[0].content.parts[0].text.strip()
            lines = [line.strip() for line in text.splitlines() if line.strip() != ""]

            score = -1
            reason = "ØªÙˆØ¶ÛŒØ­ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª"

            for line in lines:
                if line.startswith("Ø§Ù…ØªÛŒØ§Ø²"):
                    try:
                        score = int("".join(filter(str.isdigit, line)))
                    except:
                        score = -1
                if line.startswith("Ø¯Ù„ÛŒÙ„"):
                    reason = line.replace("Ø¯Ù„ÛŒÙ„:", "").strip()

# Ø§Ú¯Ø± Ø¯Ù„ÛŒÙ„ Ù‡Ù†ÙˆØ² Ø®Ø§Ù„ÛŒÙ‡ØŒ Ø³Ø¹ÛŒ Ú©Ù† Ø®Ø· Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ù…ØªÛŒØ§Ø² Ø±Ùˆ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¯Ù„ÛŒÙ„ Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒ
            if reason == "ØªÙˆØ¶ÛŒØ­ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª":
                for i, line in enumerate(lines):
                    if "Ø§Ù…ØªÛŒØ§Ø²" in line and i + 1 < len(lines):
                        possible_reason = lines[i + 1]
                        if not possible_reason.startswith("Ø§Ù…ØªÛŒØ§Ø²") and "Ø¯Ù„ÛŒÙ„" not in possible_reason:
                            reason = possible_reason
                            break

            # Ø´Ø±Ø· Ø®Ø§Øµ Ø¨Ø±Ø§ÛŒ job_spatial_01
            if job["id"] == "job_spatial_01":
                keywords = ['RS', 'GIS', 'Ù†Ù‚Ø´Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ', 'Ù†Ù‚Ø´Ù‡ Ú©Ø´ÛŒ', 'Remote Sensing', 'Geographic Information System']
                if not any(keyword.lower() in resume_text.lower() for keyword in keywords):
                    if score >= 30:
                        reason += " (Ø§Ù…ØªÛŒØ§Ø² Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØª Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ ÙÙ‚Ø¯Ø§Ù† ØªØ¬Ø±Ø¨Ù‡ ÛŒØ§ Ø¯Ø§Ù†Ø´ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ RS ÛŒØ§ GIS)"
                        score = 25  # ÛŒØ§ Ù‡Ø± Ø¹Ø¯Ø¯ Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ø§Ø² Û³Û°

                       
            return {"title": job["title"], "score": score, "reason": reason}

        except Exception as e:
            return None

    with concurrent.futures.ThreadPoolExecutor() as executor:
        future_to_job = {
            executor.submit(evaluate_job_with_key, api_key, job): job
            for api_key, job in zip(API_KEYS * (len(job_profiles) // len(API_KEYS) + 1), job_profiles)
        }

        for future in concurrent.futures.as_completed(future_to_job):
            result = future.result()
            if result:
                log_messages.append(f"ğŸ”¹ {result['title']} â†’ Ø§Ù…ØªÛŒØ§Ø²: {result['score']} | Ø¯Ù„ÛŒÙ„: {result['reason']}")
                if result["score"] > best_score:
                    best_score = result["score"]
                    best_match = result["title"]
                    best_reason = result["reason"]

    log = "\n".join(log_messages)

    if best_score >= threshold:
        return best_match, best_reason, log
    else:
        return "Ù…Ù†Ø§Ø³Ø¨ Ù‡ÛŒÚ†Ú©Ø¯Ø§Ù… Ø§Ø² Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø´ØºÙ„ÛŒ Ù†Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯", best_reason or "Ø±Ø²ÙˆÙ…Ù‡ ØªØ·Ø§Ø¨Ù‚ Ú©Ø§ÙÛŒ Ø¨Ø§ Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù… Ø§Ø² Ø´ØºÙ„â€ŒÙ‡Ø§ Ù†Ø¯Ø§Ø±Ø¯.", log




def apply_matching_to_batch(batch_df):
    matched_titles = []
    matched_reasons = []
    logs = []

    for _, row in batch_df.iterrows():
        resume_text = " ".join([str(row[col]) for col in batch_df.columns])
        title, reason, log = match_resume_to_job_parallel(resume_text, JOB_PROFILES)
        matched_titles.append(title)
        logs.append(log)
        matched_reasons.append(reason)

    batch_df["Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ"] = matched_titles
    batch_df["Ø¯Ù„ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ"] = matched_reasons
    batch_df["Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø±Ø±Ø³ÛŒ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§"] = logs
    return batch_df


# --- ØªØ§Ø¨Ø¹ Ø§ØµÙ„Ø§Ø­ Ù†Ù…Ø±Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡ ---
top_universities = ['Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØµÙ†Ø¹ØªÛŒ Ø´Ø±ÛŒÙ', 'Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù†', 'Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØµÙ†Ø¹ØªÛŒ Ø§Ù…ÛŒØ±Ú©Ø¨ÛŒØ±', 'Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø¹Ù„Ù… Ùˆ ØµÙ†Ø¹Øª Ø§ÛŒØ±Ø§Ù†']
public_keywords = ['ØµÙ†Ø¹ØªÛŒ', 'ØªÙ‡Ø±Ø§Ù†', 'Ø§Ù…ÛŒØ±Ú©Ø¨ÛŒØ±', 'Ø¹Ù„Ù… Ùˆ ØµÙ†Ø¹Øª', 'ÙØ±Ø¯ÙˆØ³ÛŒ', 'ØªØ¨Ø±ÛŒØ²', 'Ø§ØµÙÙ‡Ø§Ù†', 'Ø¯ÙˆÙ„ØªÛŒ']

def is_public_university(univ_name):
    return any(keyword in str(univ_name) for keyword in public_keywords)

def is_top_university(univ_name):
    return any(top in str(univ_name) for top in top_universities)

def color_score_column(val):
    if val >= 90:
        color = '#00C853'  # Ø³Ø¨Ø² Ù¾Ø±Ø±Ù†Ú¯
    elif val >= 80:
        color = '#AEEA00'  # Ù„ÛŒÙ…ÙˆÛŒÛŒ Ø³Ø¨Ø²
    elif val >= 70:
        color = '#FFD600'  # Ø²Ø±Ø¯
    elif val >= 60:
        color = '#FF9100'  # Ù†Ø§Ø±Ù†Ø¬ÛŒ
    elif val >= 50:
        color = '#FF3D00'  # Ù†Ø§Ø±Ù†Ø¬ÛŒ-Ù‚Ø±Ù…Ø²
    else:
        color = '#D50000'  # Ù‚Ø±Ù…Ø² ØªÛŒØ±Ù‡
    return f'background-color: {color}; color: white; font-weight: bold'


def adjust_score(row):
    score = row['score']
    if 'Ø³Ù†' in row and (row['Ø³Ù†'] < 22 or row['Ø³Ù†'] > 35):
        score -= 2
    if 'Ø­Ù‚ÙˆÙ‚ Ø¯Ø±Ø®ÙˆØ§Ø³ØªÛŒ' in row and (row['Ø­Ù‚ÙˆÙ‚ Ø¯Ø±Ø®ÙˆØ§Ø³ØªÛŒ'] < 20 or row['Ø­Ù‚ÙˆÙ‚ Ø¯Ø±Ø®ÙˆØ§Ø³ØªÛŒ'] > 45):
        score -= 10
    if 'Ù…Ù‚Ø·Ø¹ ØªØ­ØµÛŒÙ„ÛŒ' in row and 'Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ' not in str(row['Ù…Ù‚Ø·Ø¹ ØªØ­ØµÛŒÙ„ÛŒ']):
        score -= 5
    univ = row.get('Ù†Ø§Ù… Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡', '')
    if is_public_university(univ):
        score += 3
    if is_top_university(univ):
        score += 10
    return max(min(score, 100), 1.0)

def process_resume_row(row):
    resume_text = " ".join([str(row[col]) for col in row.index])
    title, reason_job, log = match_resume_to_job(resume_text, JOB_PROFILES)

    # Ú¯Ø±ÙØªÙ† Ø§Ù…ØªÛŒØ§Ø² Ø§ÙˆÙ„ÛŒÙ‡ Ø§Ø² Gemini
    try:
        gemini_df = process_batch(pd.DataFrame([row]), prompt_text="Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Ø±Ø²ÙˆÙ…Ù‡")

        if not gemini_df.empty:
            initial_score = gemini_df.iloc[0]['score'] if 'score' in gemini_df.columns else 1.0
            reason = gemini_df.iloc[0]['Ø¯Ù„ÛŒÙ„'] if 'Ø¯Ù„ÛŒÙ„' in gemini_df.columns else "ØªÙˆØ¶ÛŒØ­ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª"
        else:
            initial_score = 1.0
            reason = "Ù¾Ø§Ø³Ø®ÛŒ Ø§Ø² Ù…Ø¯Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯"

    except Exception:
        initial_score = 1.0
        reason = "Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ù…ØªÛŒØ§Ø²"

    score = adjust_score({**row.to_dict(), 'score': initial_score})

    # Ø³Ø§Ø®ØªÙ† Ø¯Ø§Ø¯Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
    row_data = row.to_dict()
    row_data.update({
        "score": score,
        "Ø¯Ù„ÛŒÙ„": reason,
        "Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ": title,
        "Ø¯Ù„ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ": reason_job,
        "Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø±Ø±Ø³ÛŒ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§": log
    })

    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„
    if RESULT_FILE_PATH.exists():
        existing = pd.read_excel(RESULT_FILE_PATH)
        updated = pd.concat([existing, pd.DataFrame([row_data])], ignore_index=True)
    else:
        updated = pd.DataFrame([row_data])

    updated.to_excel(RESULT_FILE_PATH, index=False)

    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± session_state
    st.session_state['live_results'].append(row_data)
    return row_data



    # Ø³Ù¾Ø³ Ø§ØµÙ„Ø§Ø­ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø±Ø§ÛŒØ· Ø®Ø§Øµ
    score = adjust_score({**row.to_dict(), 'score': initial_score})

    # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§ØµÙ„ÛŒ Ø±Ø²ÙˆÙ…Ù‡ + Ù†ØªØ§ÛŒØ¬ ØªØ­Ù„ÛŒÙ„ Ø±Ø§ ØªØ±Ú©ÛŒØ¨ Ú©Ù†
    new_data = row.to_dict()
    new_data.update({
        "score": score,
        "why": gemini_df.iloc[0]['Ø¯Ù„ÛŒÙ„'],
        "Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ": title,
        "Ø¯Ù„ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ": reason,
        "Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø±Ø±Ø³ÛŒ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§": log
    })

    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„
    if RESULT_FILE_PATH.exists():
        existing = pd.read_excel(RESULT_FILE_PATH)
        updated = pd.concat([existing, pd.DataFrame([new_data])], ignore_index=True)
    else:
        updated = pd.DataFrame([new_data])

    updated.to_excel(RESULT_FILE_PATH, index=False)

    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡â€ŒÛŒ session
    st.session_state['live_results'].append(new_data)
    return new_data

# --- Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ ---
st.markdown("<h1 style='color:#1a73e8; font-size: 40px;'>ğŸ“‹ Ø³Ø§Ù…Ø§Ù†Ù‡ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±Ø²ÙˆÙ…Ù‡</h1>", unsafe_allow_html=True)
st.markdown("<p style='font-size: 16px; color: #555;'>Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø±Ø²ÙˆÙ…Ù‡â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ø§Ù†Ø³Ø§Ù†ÛŒØŒ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø´ØºÙ„ÛŒ Ùˆ Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ.</p>", unsafe_allow_html=True)

uploaded_file = st.file_uploader("ğŸ“„ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø±Ø²ÙˆÙ…Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯:", type=["xlsx"])

with st.sidebar:
    st.markdown("## ğŸ“Š ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ…")
    st.markdown("### â³ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø±Ø²ÙˆÙ…Ù‡â€ŒÙ‡Ø§")
    status_placeholder = st.empty()
    progress_placeholder = st.empty()

if uploaded_file and ('live_results' not in st.session_state or len(st.session_state['live_results']) == 0):
    status_placeholder.info("âœ… ÙØ§ÛŒÙ„ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡. Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ...")
    progress_placeholder.progress(0)
elif not uploaded_file:
    status_placeholder.info("â³ Ù…Ù†ØªØ¸Ø± Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ø±Ø²ÙˆÙ…Ù‡ Ø¨Ø§Ø´ÛŒØ¯.")
    progress_placeholder.progress(0)

# Example inside your loop:
# for idx, (_, row) in enumerate(df.iterrows()):
#     ... your logic ...
#     # Update stats here:
#     live_df = pd.DataFrame(st.session_state['live_results'])
#     total = len(df)
#     checked = len(live_df)
#     accepted = (live_df['ØªØ§ÛŒÛŒØ¯ Ùˆ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡'] == 'ØªØ§ÛŒÛŒØ¯').sum() if 'ØªØ§ÛŒÛŒØ¯ Ùˆ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡' in live_df.columns else 0
#     failed = (live_df['ØªØ§ÛŒÛŒØ¯ Ùˆ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡'] != 'ØªØ§ÛŒÛŒØ¯').sum() if 'ØªØ§ÛŒÛŒØ¯ Ùˆ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡' in live_df.columns else 0
#     status_placeholder.success(f"Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø¯Ù‡: {checked} / {total}")
#     status_placeholder.markdown(f"ğŸŸ¢ Ù‚Ø¨ÙˆÙ„â€ŒØ´Ø¯Ù‡: {accepted}")
#     status_placeholder.markdown(f"ğŸ”´ Ø±Ø¯â€ŒØ´Ø¯Ù‡: {failed}")
#     progress_placeholder.progress(checked / total)



with st.sidebar:
    if st.button("ğŸ”„ Ø±ÛŒØ³Øª Ú©Ø§Ù…Ù„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª"):
        for key in ['final_df', 'live_results']:
            if key in st.session_state:
                del st.session_state[key]
        if RESULT_FILE_PATH.exists():
            RESULT_FILE_PATH.unlink()
        st.success("âœ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø±ÛŒØ³Øª Ø´Ø¯.")


prompt_text = st.text_input("ğŸ¯ Ø¹Ù†ÙˆØ§Ù† Ø´ØºÙ„ÛŒ:")
skills_text = st.text_input("ğŸ”§ Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯Ù†ÛŒØ§Ø² (Ù…Ø«Ù„Ø§Ù‹ Python, Excel):")

if uploaded_file:
    df = pd.read_excel(uploaded_file)
    df['Ø´Ù…Ø§Ø±Ù‡ ØªÙ…Ø§Ø³'] = df['Ø´Ù…Ø§Ø±Ù‡ ØªÙ…Ø§Ø³'].astype(str)
    stage = st.radio("ğŸ§© Ù…Ø±Ø­Ù„Ù‡ Ù…ÙˆØ±Ø¯Ù†Ø¸Ø± Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:", ["Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ", "ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø´ØºÙ„ÛŒ"])
    done_ids = []

    if RESULT_FILE_PATH.exists():
        done_ids = pd.read_excel(RESULT_FILE_PATH)['Ø´Ù†Ø§Ø³Ù‡'].tolist()

    if stage == "Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ":
        st.markdown("### ğŸš€ Ù…Ø±Ø­Ù„Ù‡ Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø±Ø²ÙˆÙ…Ù‡â€ŒÙ‡Ø§")
        if st.button("Ø´Ø±ÙˆØ¹ Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ"):
            results_placeholder = st.empty()
            progress_bar = st.progress(0)
            for idx, (_, row) in enumerate(df.iterrows()):
                if row[ID_COLUMN] in done_ids:
                    continue

                gemini_df = process_batch(pd.DataFrame([row]), prompt_text="Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Ø±Ø²ÙˆÙ…Ù‡")
                initial_score = gemini_df.iloc[0]['score']
                score = adjust_score({**row.to_dict(), 'score': initial_score})
                row_data = row.to_dict()
                row_data.update({
                    "score": score,
                    "why": gemini_df.iloc[0]['Ø¯Ù„ÛŒÙ„'] if not gemini_df.empty and 'Ø¯Ù„ÛŒÙ„' in gemini_df.columns else "Ø¯Ù„ÛŒÙ„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯"
                })

                if 'ØªØ§ÛŒÛŒØ¯ Ùˆ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡' in row and row['ØªØ§ÛŒÛŒØ¯ Ùˆ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡'] in ['ØªØ§ÛŒÛŒØ¯', 'Ø±Ø¯']:
                    row_data['ØªØ§ÛŒÛŒØ¯ Ùˆ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡'] = row['ØªØ§ÛŒÛŒØ¯ Ùˆ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡']      

                if RESULT_FILE_PATH.exists():
                    existing = pd.read_excel(RESULT_FILE_PATH)
                    updated = pd.concat([existing, pd.DataFrame([row_data])], ignore_index=True)
                else:
                    updated = pd.DataFrame([row_data])
                updated.to_excel(RESULT_FILE_PATH, index=False)

                st.session_state['live_results'].append(row_data)

                results_df = pd.DataFrame(st.session_state['live_results'])
                results_placeholder.dataframe(results_df)

    # ----- Dynamic sidebar updates -----
                live_df = results_df
                total = len(df)
                checked = len(live_df)
                accepted = (live_df['ØªØ§ÛŒÛŒØ¯ Ùˆ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡'] == 'ØªØ§ÛŒÛŒØ¯').sum() if 'ØªØ§ÛŒÛŒØ¯ Ùˆ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡' in live_df.columns else 0
                failed = (live_df['ØªØ§ÛŒÛŒØ¯ Ùˆ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡'] != 'ØªØ§ÛŒÛŒØ¯').sum() if 'ØªØ§ÛŒÛŒØ¯ Ùˆ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡' in live_df.columns else 0

                status_placeholder.success(f"Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø¯Ù‡: {checked} / {total}")
                status_placeholder.markdown(f"ğŸŸ¢ Ù‚Ø¨ÙˆÙ„â€ŒØ´Ø¯Ù‡: {accepted}")
                status_placeholder.markdown(f"ğŸ”´ Ø±Ø¯â€ŒØ´Ø¯Ù‡: {failed}")
                progress_placeholder.progress(checked / total)
    # -----------------------------------

                progress_bar.progress((idx + 1) / len(df))
                time.sleep(1.5)


            st.success("âœ… Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")

    elif stage == "ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø´ØºÙ„ÛŒ":
        st.markdown("### ğŸ” Ù…Ø±Ø­Ù„Ù‡ ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø´ØºÙ„ÛŒ")
        results_placeholder = st.empty()
        progress_bar = st.progress(0)

        if 'ØªØ§ÛŒÛŒØ¯ Ùˆ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡' in df.columns:
            initial_count = len(df)
        df = df[df['ØªØ§ÛŒÛŒØ¯ Ùˆ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡'] == 'ØªØ§ÛŒÛŒØ¯']
        removed_count = initial_count - len(df)
        st.info(f"ğŸ“¤ {removed_count} Ø±Ø²ÙˆÙ…Ù‡ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡ Ø­Ø°Ù Ø´Ø¯Ù†Ø¯. {len(df)} Ø±Ø²ÙˆÙ…Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§Ù‚ÛŒ Ù…Ø§Ù†Ø¯.")


        if st.button("ğŸš€Ø´Ø±ÙˆØ¹ ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø´ØºÙ„ÛŒ"):
            try:
            # No need to read df again, just use df already defined above
                match_results = apply_matching_to_batch(df.copy())
                match_result_file = "job_matching_results.xlsx"
                match_results.to_excel(match_result_file, index=False)
                st.success("âœ… ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø´ØºÙ„ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
                st.dataframe(match_results)
                with open(match_result_file, "rb") as f:
                    st.download_button("ğŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†ØªØ§ÛŒØ¬ ØªØ·Ø¨ÛŒÙ‚ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡", f, file_name=match_result_file)
            except Exception as e:
                st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ù†Ø¬Ø§Ù… ØªØ·Ø¨ÛŒÙ‚: {e}")


            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯
                for col in ["Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ", "Ø¯Ù„ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ", "Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø±Ø±Ø³ÛŒ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§"]:
                    if col not in df.columns:
                        df[col] = None

                rows_to_process = df[df["Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ"].isna()]
                progress_bar = st.progress(0)
                results = []

                for idx, (_, row) in enumerate(rows_to_process.iterrows()):
                    resume_text = " ".join([str(row[col]) for col in df.columns])
                    title, reason, log = match_resume_to_job(resume_text, JOB_PROFILES)

                    row["Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ"] = title
                    row["Ø¯Ù„ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ"] = reason
                    row["Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø±Ø±Ø³ÛŒ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§"] = log
                    results.append(row)

                    progress_bar.progress((idx + 1) / len(rows_to_process))
                    time.sleep(4)

            # ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ Ø¨Ø§Ù‚ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
                rest_df = df[~df.index.isin(rows_to_process.index)]
                final_df = pd.concat([rest_df, pd.DataFrame(results)], ignore_index=True)

            # Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
                final_df.to_excel(RESULT_FILE_PATH, index=False)

                st.success("âœ… ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø´ØºÙ„ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
                st.download_button("ğŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†ØªØ§ÛŒØ¬ ØªØ·Ø¨ÛŒÙ‚", data=open(RESULT_FILE_PATH, "rb").read(), file_name="matched_resumes.xlsx")


            # âœ… Ù†Ù…Ø§ÛŒØ´ Ø²Ù†Ø¯Ù‡ Ø¬Ø¯ÙˆÙ„ Ù¾Ø³ Ø§Ø² Ù‡Ø± Ø±Ø²ÙˆÙ…Ù‡
            if 'live_results' in st.session_state:
                results_df = pd.DataFrame(st.session_state['live_results'])
                live_columns = [
                    'Ø´Ù†Ø§Ø³Ù‡', 'Ù†Ø§Ù…', 'Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ', 'ØªØ§ÛŒÛŒØ¯ Ùˆ Ø±Ø¯ Ø§ÙˆÙ„ÛŒÙ‡', 'Ø¹Ù„Øª Ø±Ø¯',
                    'score', 'Ø¯Ù„ÛŒÙ„', 'Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ', 'Ø¯Ù„ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ',
                    'Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø±Ø±Ø³ÛŒ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§'
                ]
                live_columns_available = [col for col in live_columns if col in results_df.columns]
                display_live_df = results_df[live_columns_available].copy()
                display_live_df.index = display_live_df.index + 1
                display_live_df.index.name = "Ø±Ø¯ÛŒÙ"
                results_placeholder.dataframe(display_live_df)

            # ğŸ”„ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†ÙˆØ§Ø± Ù¾ÛŒØ´Ø±ÙØª
            progress_bar.progress(1.0)

            # â±ï¸ ØªØ§Ø®ÛŒØ± Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overload
            time.sleep(2)

            # Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ù†Ù‡Ø§ÛŒÛŒ

            # ØªØ¹ÛŒÛŒÙ† Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´ØºÙ„ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ùˆ Ø¯Ù„ÛŒÙ„

            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± session_state

# --- Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ ---
# --- Ù†Ù…Ø§ÛŒØ´ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ú©Ø§Ù…Ù„ ---
# --- Ù†Ù…Ø§ÛŒØ´ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ú©Ø§Ù…Ù„ ---
if RESULT_FILE_PATH.exists():
    final_df = pd.read_excel(RESULT_FILE_PATH)

    # Ù†Ù…Ø§ÛŒØ´ Ú©Ø§Ù…Ù„ Ù‡Ù…Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ø¯ÙˆÙ† Ù…Ø­Ø¯ÙˆØ¯Ø³Ø§Ø²ÛŒ
    display_df = final_df.copy()
    display_df.index = display_df.index + 1
    display_df.index.name = "Ø±Ø¯ÛŒÙ"

    st.markdown("### âœ… Ø¬Ø¯ÙˆÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ø±Ø²ÙˆÙ…Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒâ€ŒØ´Ø¯Ù‡")
    
    # Ø§Ú¯Ø± Ø³ØªÙˆÙ† 'score' ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªØŒ Ø±Ù†Ú¯â€ŒØ¢Ù…ÛŒØ²ÛŒ Ú©Ù†
    if 'score' in display_df.columns:
        styled_df = display_df.style.applymap(color_score_column, subset=['score'])
        st.dataframe(styled_df)
    else:
        st.dataframe(display_df)

    # Ø¯Ú©Ù…Ù‡ Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ù†Ù‡Ø§ÛŒÛŒ
    with open(RESULT_FILE_PATH, "rb") as f:
        st.download_button("ğŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ù†Ù‡Ø§ÛŒÛŒ", f, file_name="resume_results.xlsx")
